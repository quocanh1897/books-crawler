# Cleanup Plan

Audit of what can be safely deleted now that `book-ingest` is the primary pipeline for MTC books.

## Current Workflow

| Task | Tool | Notes |
|------|------|-------|
| Generate `fresh_books_download.json` | `crawler-descryptor/fetch_catalog.py` | Paginates API, cross-refs local data, outputs plan |
| Download + decrypt + compress + DB | `book-ingest/ingest.py` | Reads plan file, writes bundles + SQLite directly |
| Pull covers | `meta-puller/pull_metadata.py --cover-only` | Downloads to `binslib/public/covers/` |
| Sync files across machines | `sync-book/sync-bundles.sh` | Bundles + covers via rsync |
| TTV book crawling | `crawler-tangthuvien/` | Still active, separate source |
| TTV import to DB | `binslib/scripts/import.ts` | Reads `.txt` from `crawler-tangthuvien/output/` |
| EPUB generation | `epub-converter/` | Reads from `crawler/output/` |
| DB migrations | `binslib/scripts/migrate.ts` | Drizzle + FTS5 setup |

---

## Dependency Audit

### 1. `crawler/` — ⚠️ DATA directory, NOT safe to delete

The `crawler/` directory no longer contains any crawler code (that moved to `crawler-emulator/`). It is now purely a **data mount point**: `crawler/output/` holds 30,354 book directories with `.txt` chapters, `metadata.json`, and `cover.jpg` files.

**Referenced by** (hardcoded `../../crawler/output` or env `CRAWLER_OUTPUT_DIR`):

| File | How |
|------|-----|
| `binslib/scripts/import.ts` | Scans for MTC `.txt` chapters + metadata |
| `binslib/scripts/pre-compress.ts` | Reads `.txt` to build bundles |
| `binslib/scripts/audit.ts` | Cross-refs crawler output vs bundles |
| `binslib/scripts/is-safe-to-delete.ts` | Verifies `.txt` source exists for every bundle chapter |
| `binslib/scripts/pull-covers.ts` | Reads `metadata.json` for poster URLs |
| `binslib/scripts/test-export-book.ts` | Reads `.txt` for test compression |
| `binslib/docker-compose.yml` | Volume mount `../crawler/output:/data/crawler-output` |
| `meta-puller/pull_metadata.py` | `OUTPUT_DIR = ../crawler/output` — scans + writes metadata/covers |
| `epub-converter/convert.py` | `OUTPUT_DIR = crawler/output` — reads chapters for EPUB |
| `epub-converter/docker-compose.yml` | Volume mount `../crawler:/data/crawler` |
| `crawler-descryptor/fetch_catalog.py` | `OUTPUT_DIR = ../crawler/output` — counts local chapters |

**Verdict**: **Cannot delete `crawler/output/`**. It is the canonical MTC chapter source used by the import pipeline, epub-converter, meta-puller, and all audit scripts. The only deletable content is `crawler/zstd-benchmark/` (standalone benchmarking tool, not referenced anywhere).

**Future**: Once all MTC books are fully ingested via `book-ingest` (bundles contain all data including metadata), the `.txt` files become redundant. But `epub-converter` and `binslib/scripts/import.ts` (for TTV) still expect this directory structure. A future phase could:
1. Run `is-safe-to-delete.ts` to verify all `.txt` chapters exist in bundles
2. Delete individual `.txt` files (keep `metadata.json` + `cover.jpg` for epub-converter)
3. Or update epub-converter to read from bundles instead of `.txt`

---

### 2. `crawler-descryptor/` — ✅ Safe to delete WITH prep work

Most code is superseded by `book-ingest`, which has its own copies of the API client, decryption, and download logic. However, there are **three critical dependencies** that must be resolved first.

#### Dependency A: `src/decrypt.py` symlink

```
book-ingest/src/decrypt.py -> ../../crawler-descryptor/src/decrypt.py
```

`book-ingest` uses a **symlink** to the decryption module. Deleting `crawler-descryptor/` would break `book-ingest` entirely.

**Fix**: Replace the symlink with a real file copy.

```bash
cd book-ingest/src
rm decrypt.py                                          # remove symlink
cp ../../crawler-descryptor/src/decrypt.py decrypt.py  # copy real file
```

#### Dependency B: `fresh_books_download.json` (plan file)

```python
# book-ingest/ingest.py line 67
DEFAULT_PLAN = SCRIPT_DIR.parent / "crawler-descryptor" / "fresh_books_download.json"
```

`book-ingest` defaults to reading its plan from `crawler-descryptor/`. The file is generated by `fetch_catalog.py`.

**Fix**: Move the plan file and update the default path.

```bash
mv crawler-descryptor/fresh_books_download.json book-ingest/data/
```

```python
# book-ingest/ingest.py — update DEFAULT_PLAN
DEFAULT_PLAN = SCRIPT_DIR / "data" / "fresh_books_download.json"
```

#### Dependency C: `fetch_catalog.py` (plan generator)

`fetch_catalog.py` is the **only** tool that generates `fresh_books_download.json`. It paginates the API catalog, cross-references with local data (crawler output + bundles), and outputs the plan. `book-ingest` does not have this capability.

**Fix**: Move `fetch_catalog.py` into `book-ingest/` and adapt its imports.

Files to move:
- `crawler-descryptor/fetch_catalog.py` → `book-ingest/fetch_catalog.py`
- `crawler-descryptor/src/utils.py` → `book-ingest/src/utils.py` (used by `fetch_catalog.py` for `count_existing_chapters`, `read_bundle_indices`)

Update imports in `fetch_catalog.py`:
- Replace `from config import BASE_URL, HEADERS` with inline constants (already defined in `book-ingest/src/api.py`)
- Replace `from src.utils import count_existing_chapters` path

#### Dependency D: `meta-puller/pull_metadata.py` config import

```python
# meta-puller/pull_metadata.py line 38
sys.path.insert(0, os.path.join(os.path.dirname(__file__), "..", "crawler"))
from config import BASE_URL, HEADERS, REQUEST_DELAY
```

This is **already broken** — it points to `../crawler/` but the config lives in `../crawler-descryptor/`. We've been working around it with `PYTHONPATH="../crawler-descryptor"`.

**Fix**: Give `meta-puller` its own inline config (3 constants) instead of importing from another subproject.

```python
# meta-puller/pull_metadata.py — replace sys.path hack with inline config
import os

BASE_URL = "https://android.lonoapp.net"
BEARER_TOKEN = os.environ.get(
    "MTC_BEARER_TOKEN",
    "7045826|W0GmBOqfeWO0wWZUD7QpikPjvMsP1tq7Ayjq48pX",
)
HEADERS = {
    "authorization": f"Bearer {BEARER_TOKEN}",
    "x-app": "app.android",
    "user-agent": "Dart/3.5 (dart:io)",
    "content-type": "application/json",
}
REQUEST_DELAY = 0.3
```

#### What gets deleted

After the prep work, everything remaining in `crawler-descryptor/` is superseded:

| File | Superseded by |
|------|---------------|
| `config.py` | `book-ingest/src/api.py` inline constants + `meta-puller` inline |
| `main.py` | `book-ingest/ingest.py` (single book) |
| `download_batch.py` | `book-ingest/ingest.py -w N` |
| `download_topN.py` | `book-ingest/ingest.py --plan ...` |
| `collect_samples.py` | One-time analysis tool, no longer needed |
| `src/client.py` | `book-ingest/src/api.py` `AsyncBookClient` |
| `src/downloader.py` | `book-ingest/src/api.py` |
| `src/decrypt.py` | Copied into `book-ingest/src/decrypt.py` |
| `src/iv_extract.py` | One-time analysis tool |
| `src/utils.py` | Moved to `book-ingest/src/utils.py` |
| `fetch_catalog.py` | Moved to `book-ingest/fetch_catalog.py` |
| `frida/` | Reverse-engineering tools, historical only |
| `tests/` | Tests for the old client |
| `API.md`, `ENCRYPTION.md`, `KNOWLEDGE.md` | Archive to `plan/archive/` or `book-ingest/docs/` |

**Verdict**: Safe to delete **after** completing all four prep steps. Total risk: low — all runtime dependencies are being moved/copied, not removed.

---

### 3. `crawler-emulator/` — ✅ Safe to delete immediately

The emulator-based crawler is the original approach, deprecated since `crawler-descryptor` and now fully replaced by `book-ingest`.

**Dependencies**: Zero. No other subproject imports from, symlinks to, or references `crawler-emulator/`.

**Contents**: `grab_book.py`, `parallel_grab.py`, `batch_grab.py`, `extract_book.py`, `start_emulators.sh`, `config.py`, `progress-checking/`, docs, APK directory.

**Verdict**: **Safe to delete immediately**. No prep work needed. The README already marks it as `[DEPRECATED]`.

---

### 4. `binslib/scripts/` — ⚠️ Partial deletion only

| Script | Status | Reason |
|--------|--------|--------|
| `migrate.ts` | **KEEP** | Essential — runs Drizzle migrations + FTS5 table creation |
| `import.ts` | **KEEP** | Still needed for TTV books (reads `crawler-tangthuvien/output/`). Also serves as the only way to import `.txt` chapters from disk into bundles + DB for any source |
| `pre-compress.ts` | **KEEP for now** | Useful for TTV `.txt` → bundle conversion. Redundant for MTC (book-ingest compresses inline) |
| `audit.ts` | **KEEP** | Diagnostic tool — audits crawler output vs bundles. No replacement in book-ingest |
| `is-safe-to-delete.ts` | **KEEP** | Safety tool — verifies all `.txt` chapters exist in bundles before deletion. Will be needed when cleaning up `crawler/output/` `.txt` files |
| `warmup-cache.js` | **KEEP** | Production deployment utility — warms OS page cache for SQLite |
| `pull-covers.ts` | **DELETE** | Fully superseded by `meta-puller/pull_metadata.py --cover-only`. Same logic (read metadata.json poster → download), but the Python version is now the canonical tool |
| `test-export-book.ts` | **DELETE** | One-time test script for zstd compression. Not part of any workflow |

**Verdict**: Delete `pull-covers.ts` and `test-export-book.ts`. Keep everything else.

---

## Execution Plan

### Phase 1: Immediate deletions (zero risk)

```bash
# Delete deprecated emulator crawler
rm -rf crawler-emulator/

# Delete superseded binslib scripts
rm binslib/scripts/pull-covers.ts
rm binslib/scripts/test-export-book.ts

# Delete standalone benchmark (not referenced)
rm -rf crawler/zstd-benchmark/
```

### Phase 2: Migrate dependencies out of `crawler-descryptor/`

**Step 2a** — Break the decrypt.py symlink:

```bash
rm book-ingest/src/decrypt.py
cp crawler-descryptor/src/decrypt.py book-ingest/src/decrypt.py
```

**Step 2b** — Move fetch_catalog.py + utils.py into book-ingest:

```bash
cp crawler-descryptor/fetch_catalog.py book-ingest/fetch_catalog.py
cp crawler-descryptor/src/utils.py book-ingest/src/utils.py
```

Then edit `book-ingest/fetch_catalog.py`:
- Replace `from config import BASE_URL, HEADERS` with imports from `src.api`
- Replace `from src.utils import count_existing_chapters` (path now local)
- Update `OUTPUT_DIR` to point to `../crawler/output`
- Update `CATALOG_FILE` and plan output paths

**Step 2c** — Move plan file + update default path:

```bash
mkdir -p book-ingest/data
mv crawler-descryptor/fresh_books_download.json book-ingest/data/
```

Edit `book-ingest/ingest.py` line 67:
```python
DEFAULT_PLAN = SCRIPT_DIR / "data" / "fresh_books_download.json"
```

**Step 2d** — Inline config in meta-puller:

Edit `meta-puller/pull_metadata.py`:
- Remove `sys.path.insert(0, ...)` and `from config import ...`
- Add inline `BASE_URL`, `HEADERS`, `REQUEST_DELAY` constants
- This also fixes the existing broken import path (`../crawler` → should be `../crawler-descryptor`)

**Step 2e** — Archive docs:

```bash
mkdir -p plan/archive/crawler-descryptor
cp crawler-descryptor/API.md plan/archive/crawler-descryptor/
cp crawler-descryptor/ENCRYPTION.md plan/archive/crawler-descryptor/
cp crawler-descryptor/KNOWLEDGE.md plan/archive/crawler-descryptor/
```

### Phase 3: Verify and delete `crawler-descryptor/`

```bash
# Verify book-ingest still works without crawler-descryptor
cd book-ingest
python3 -c "from src.decrypt import decrypt_content; print('decrypt OK')"
python3 -c "from src.utils import count_existing_chapters; print('utils OK')"
python3 ingest.py --dry-run 100358

# Verify meta-puller works standalone
cd ../meta-puller
python3 pull_metadata.py --dry-run --ids 100358

# Verify fetch_catalog imports work
cd ../book-ingest
python3 -c "import fetch_catalog; print('catalog OK')"

# If all pass, delete
rm -rf crawler-descryptor/
```

### Phase 4: Update docs and references

- Update `README.md` architecture diagram: remove `crawler/` and `crawler-descryptor/` from crawler layer
- Update `README.md` subprojects table: remove deleted entries
- Update `README.md` quick start: remove crawler-descryptor examples
- Update `binslib/docker-compose.yml`: remove `../crawler/output` mount if no longer needed in Docker context (or keep for epub-converter compatibility)
- Update `epub-converter/docker-compose.yml`: verify it still works with `../crawler` mount (only needs `output/`, not code)

---

## Dependency Graph (after cleanup)

```
book-ingest/
  ├── fetch_catalog.py        ← moved from crawler-descryptor
  ├── ingest.py               ← reads plan from data/
  ├── data/
  │   └── fresh_books_download.json  ← moved from crawler-descryptor
  └── src/
      ├── api.py              ← self-contained (own config + client)
      ├── decrypt.py           ← real file (was symlink)
      ├── utils.py             ← moved from crawler-descryptor/src
      ├── bundle.py
      ├── compress.py
      ├── cover.py
      └── db.py

meta-puller/
  └── pull_metadata.py        ← self-contained (inline config)

crawler/output/               ← data only, no code
  └── {book_id}/

crawler-tangthuvien/          ← still active (TTV source)
  └── output/{book_id}/

binslib/
  ├── scripts/
  │   ├── import.ts           ← kept (TTV import + .txt fallback)
  │   ├── migrate.ts          ← kept (essential)
  │   ├── pre-compress.ts     ← kept (TTV bundles)
  │   ├── audit.ts            ← kept (diagnostics)
  │   ├── is-safe-to-delete.ts ← kept (safety checks)
  │   └── warmup-cache.js     ← kept (production)
  └── ...

sync-book/
  └── sync-bundles.sh         ← bundles + covers

epub-converter/               ← reads crawler/output/ (data only)
```

---

## Risk Assessment

| Action | Risk | Mitigation |
|--------|------|------------|
| Delete `crawler-emulator/` | None | No references anywhere |
| Delete `pull-covers.ts`, `test-export-book.ts` | None | Superseded, not in any workflow |
| Delete `crawler/zstd-benchmark/` | None | Standalone, not referenced |
| Break decrypt.py symlink | Low | Copy before delete, verify import |
| Move `fetch_catalog.py` | Low | Update imports, test with `--dry-run` |
| Move `fresh_books_download.json` | Low | Update one path constant |
| Inline meta-puller config | Low | Fixes an existing broken import |
| Delete `crawler-descryptor/` | Low | Only after all above verified |
| Delete `crawler/output/` `.txt` files | **High** | NOT in this plan — requires `is-safe-to-delete.ts` audit per book, and epub-converter still reads `.txt` |