#!/usr/bin/env python3
"""book-ingest — Unified crawl-compress-import pipeline for multiple sources.

Sources:
    mtc (default)       metruyencv.com — encrypted mobile API, linked-list
                        chapter walk, AES-128-CBC decryption.
    ttv                 truyen.tangthuvien.vn — public HTML scraping,
                        sequential chapter URLs, no decryption.
    tf                  truyenfull.vision — public HTML scraping,
                        sequential chapter URLs, completed hot books only.

All sources share the same output pipeline: fetch → compress (zstd +
global dict) → write BLIB v2 bundles → upsert SQLite metadata.

Modes:
    Ingest (default)    Fetch new chapters from the source, compress, and
                        write to bundles + SQLite.  Skips books whose
                        bundle already has >= chapter_count chapters.
    Fix (--fix)         Re-download missing chapters for existing books.
                        Audits each bundle for gaps and fills them.
                        Bypasses the "bundle complete" skip logic.
    Force (--force)     Delete existing bundle + DB chapters, then
                        re-download everything from scratch.  Requires
                        explicit book IDs and prompts for confirmation.
    Audit (--audit-only)
                        Report missing chapters/metadata without downloading.
    Update metadata (--update-meta-only)
                        Bulk-update book metadata (author, stats, genres,
                        tags) from the plan file without downloading chapters.
                        Generates synthetic authors from creators for books
                        that have no author (id = 999{creator_id}).

Usage:
    # ── Ingest (MTC — default) ────────────────────────────────
    python3 ingest.py                           # ingest from default plan file
    python3 ingest.py 100358 100441             # specific book IDs
    python3 ingest.py -w 5                      # 5 parallel workers
    python3 ingest.py --plan custom.json        # custom plan file
    python3 ingest.py --offset 500 --limit 200  # skip first 500, process next 200
    python3 ingest.py --min-chapters 200        # skip books with < 200 chapters
    python3 ingest.py --flush-every 50          # checkpoint every 50 chapters
    python3 ingest.py --dry-run                 # simulate without writing

    # ── Ingest (TTV) ─────────────────────────────────────────
    python3 ingest.py --source ttv              # ingest from TTV plan file
    python3 ingest.py --source ttv 10000001     # specific TTV book ID
    python3 ingest.py --source ttv -w 3         # TTV with 3 workers

    # ── Ingest (TF — TruyenFull) ─────────────────────────────
    python3 ingest.py --source tf               # ingest from TF plan file
    python3 ingest.py --source tf 30000001      # specific TF book ID
    python3 ingest.py --source tf -w 3          # TF with 3 workers

    # ── Fix (fill chapter gaps) ───────────────────────────────
    python3 ingest.py --fix                     # fix all books in MTC plan
    python3 ingest.py --source tf --fix         # fix all TF plan books
    python3 ingest.py --source tf --fix 30000003  # fix specific TF book
    python3 ingest.py --fix --dry-run           # preview gaps without downloading

    # ── Force (re-download from scratch) ──────────────────────
    python3 ingest.py --force 100358 100441     # delete + re-download (prompts)
    python3 ingest.py --source tf --force 30000001

    # ── Audit ─────────────────────────────────────────────────
    python3 ingest.py --audit-only              # report gaps, no downloads

    # ── Update metadata ──────────────────────────────────────
    python3 ingest.py --update-meta-only        # update all books from plan
    python3 ingest.py --update-meta-only --limit 100  # update first 100 only

Plan files (generated by generate_plan.py):
    MTC default: data/books_plan_mtc.json
    TTV default: data/books_plan_ttv.json
    TF  default: data/books_plan_tf.json
    Books with < 100 chapters are filtered out by default (--min-chapters).
"""

from __future__ import annotations

import argparse
import asyncio
import json
import os
import sys
import time
from datetime import datetime
from pathlib import Path

from rich.console import Console
from rich.progress import (
    BarColumn,
    MofNCompleteColumn,
    Progress,
    SpinnerColumn,
    TaskProgressColumn,
    TextColumn,
    TimeElapsedColumn,
    TimeRemainingColumn,
)
from src.api import AsyncBookClient
from src.bundle import (
    ChapterMeta,
    read_bundle_indices,
    read_bundle_meta,
    read_bundle_raw,
    write_bundle,
)
from src.compress import ChapterCompressor
from src.db import (
    compute_meta_hash,
    get_book_meta_hash,
    get_chapter_indices,
    insert_chapters,
    open_db,
    update_cover_url,
    upsert_book_metadata,
)
from src.sources import VALID_SOURCES, create_source

# ─── Paths ────────────────────────────────────────────────────────────────────

SCRIPT_DIR = Path(__file__).resolve().parent
BINSLIB_DIR = SCRIPT_DIR.parent / "binslib"
COMPRESSED_DIR = BINSLIB_DIR / "data" / "compressed"
DB_PATH = BINSLIB_DIR / "data" / "binslib.db"
DICT_PATH = BINSLIB_DIR / "data" / "global.dict"
COVERS_DIR = BINSLIB_DIR / "public" / "covers"
PLAN_PREFIX = "books_plan_"
DEFAULT_PLAN = SCRIPT_DIR / "data" / "books_plan_mtc.json"
TTV_DEFAULT_PLAN = SCRIPT_DIR / "data" / "books_plan_ttv.json"
TF_DEFAULT_PLAN = SCRIPT_DIR / "data" / "books_plan_tf.json"

LOG_DIR = SCRIPT_DIR / "data"
DETAIL_LOG = LOG_DIR / "ingest-detail.log"
SUMMARY_LOG = LOG_DIR / "ingest-log.txt"
AUDIT_LOG = LOG_DIR / "audit.log"

console = Console()

# ─── Source-specific defaults ─────────────────────────────────────────────────

_SOURCE_DEFAULTS = {
    "mtc": {"max_concurrent": 180, "request_delay": 0.015},
    "ttv": {"max_concurrent": 20, "request_delay": 0.3},
    "tf": {"max_concurrent": 20, "request_delay": 0.3},
}

# ─── Helpers ──────────────────────────────────────────────────────────────────


def timestamp() -> str:
    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")


def format_duration(seconds: float) -> str:
    s = int(seconds)
    if s < 60:
        return f"{s}s"
    m, sec = divmod(s, 60)
    if m < 60:
        return f"{m}m {sec}s"
    h, m = divmod(m, 60)
    return f"{h}h {m}m {sec}s"


def format_num(n: int) -> str:
    return f"{n:,}"


def log_detail(msg: str) -> None:
    LOG_DIR.mkdir(parents=True, exist_ok=True)
    with open(DETAIL_LOG, "a") as f:
        f.write(f"[{timestamp()}] {msg}\n")


def log_summary(msg: str) -> None:
    LOG_DIR.mkdir(parents=True, exist_ok=True)
    with open(SUMMARY_LOG, "a") as f:
        f.write(f"[{timestamp()}] {msg}\n")


# ─── Plan File Loading ────────────────────────────────────────────────────────


def load_plan(plan_path: str, offset: int = 0, limit: int = 0) -> list[dict]:
    """Load book entries from a plan JSON file.

    Supports both flat arrays and structured plans with need_download/partial.
    """
    with open(plan_path) as f:
        data = json.load(f)

    entries: list[dict] = []
    if isinstance(data, list):
        entries = data
    elif isinstance(data, dict):
        for key in ("need_download", "partial", "have_partial", "books"):
            if key in data and isinstance(data[key], list):
                entries.extend(data[key])

    if offset > 0:
        entries = entries[offset:]
    if limit > 0:
        entries = entries[:limit]

    return entries


def entries_from_ids(book_ids: list[int], source_name: str = "mtc") -> list[dict]:
    """Create minimal plan entries from explicit book IDs.

    For TTV books the slug is required (URLs are slug-based).  If the
    book ID exists in the TTV registry we attach it; otherwise the source
    will skip the entry with a warning.
    """
    entries = [{"id": bid} for bid in book_ids]

    if source_name == "ttv":
        from src.sources.ttv import load_registry

        registry = load_registry()
        # Invert: id → slug
        id_to_slug = {v: k for k, v in registry.items()}
        for e in entries:
            slug = id_to_slug.get(e["id"])
            if slug:
                e["slug"] = slug

    elif source_name == "tf":
        from src.sources.tf import load_registry as load_tf_registry

        registry = load_tf_registry()
        # Invert: id → tf_slug
        id_to_slug = {v: k for k, v in registry.items()}
        for e in entries:
            tf_slug = id_to_slug.get(e["id"])
            if tf_slug:
                e["tf_slug"] = tf_slug
                e["slug"] = tf_slug  # fallback for fetch_book_metadata

    return entries


# ─── Audit Mode ───────────────────────────────────────────────────────────────


async def run_audit(entries: list[dict], client: AsyncBookClient) -> None:
    """Audit mode: report missing books/chapters without downloading."""
    console.print("\n[bold]AUDIT MODE[/bold] — scanning for gaps...\n")

    total = len(entries)
    complete = 0
    missing_chapters: list[
        tuple[int, str, int, int, int]
    ] = []  # (id, name, bundle, api, missing)
    not_in_db = 0
    missing_covers = 0

    db = open_db(str(DB_PATH))

    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        MofNCompleteColumn(),
        TaskProgressColumn(),
        TimeElapsedColumn(),
        TimeRemainingColumn(),
        console=console,
    ) as progress:
        task = progress.add_task("Auditing books", total=total)

        for entry in entries:
            book_id = entry["id"]
            name = entry.get("name", "?")

            # Check bundle
            bundle_path = str(COMPRESSED_DIR / f"{book_id}.bundle")
            bundle_indices = read_bundle_indices(bundle_path)

            # Get API chapter count (from plan or fetch)
            api_count = entry.get("chapter_count", 0)
            if not api_count:
                try:
                    meta = await client.get_book(book_id)
                    api_count = meta.get("chapter_count", 0)
                    name = meta.get("name", name)
                except Exception:
                    pass

            bundle_count = len(bundle_indices)

            if bundle_count >= api_count and api_count > 0:
                complete += 1
            elif api_count > 0:
                gap = api_count - bundle_count
                missing_chapters.append((book_id, name, bundle_count, api_count, gap))

            # Check DB
            cur = db.execute("SELECT id FROM books WHERE id = ?", (book_id,))
            if not cur.fetchone():
                not_in_db += 1

            # Check cover
            cover_path = COVERS_DIR / f"{book_id}.jpg"
            if not cover_path.exists():
                missing_covers += 1

            progress.update(task, advance=1)

    db.close()

    # Report
    report_lines = [
        "",
        "  AUDIT REPORT",
        "  " + "─" * 40,
        f"  Total books in plan:         {format_num(total):>8}",
        f"  Books with complete data:    {format_num(complete):>8}",
        f"  Books with missing chapters: {format_num(len(missing_chapters)):>8}",
    ]

    # Show top missing books
    missing_chapters.sort(key=lambda x: x[4], reverse=True)
    for book_id, name, bundle, api, gap in missing_chapters[:20]:
        report_lines.append(
            f"    Book {book_id}: bundle={bundle}, API={api}, missing={gap}"
        )
    if len(missing_chapters) > 20:
        report_lines.append(f"    ... and {len(missing_chapters) - 20} more")

    report_lines.extend(
        [
            f"  Books not in DB:             {format_num(not_in_db):>8}",
            f"  Books missing covers:        {format_num(missing_covers):>8}",
            "  " + "─" * 40,
            "",
        ]
    )

    report_text = "\n".join(report_lines)
    console.print(report_text)

    # Write to audit log
    LOG_DIR.mkdir(parents=True, exist_ok=True)
    with open(AUDIT_LOG, "w") as f:
        f.write(f"Audit run at {timestamp()}\n")
        f.write(report_text + "\n")
        f.write("\nFull missing chapters list:\n")
        for book_id, name, bundle, api, gap in missing_chapters:
            f.write(f"  {book_id}\t{name}\tbundle={bundle}\tAPI={api}\tmissing={gap}\n")

    console.print(f"  Full report written to: {AUDIT_LOG}\n")


# ─── Per-Book Ingest ──────────────────────────────────────────────────────────


async def ingest_book(
    source,
    entry: dict,
    compressor: ChapterCompressor,
    db_path: str,
    flush_every: int,
    dry_run: bool,
    book_progress: Progress,
    book_task_id: int,
    chapter_progress: Progress,
    chapter_task_id: int,
    lock: asyncio.Lock,
    fix_mode: bool = False,
) -> dict:
    """Ingest a single book: fetch → compress → bundle + DB.

    The *source* handles all transport details (API calls, HTML parsing,
    decryption, walk strategy).  This function is source-agnostic.

    When *fix_mode* is True, the "bundle complete" skip logic is bypassed
    so that missing chapters (gaps in the bundle) are re-downloaded.

    Returns stats dict with keys: book_id, name, saved, skipped, errors.
    """
    book_id = entry["id"]
    stats = {
        "book_id": book_id,
        "name": entry.get("name", "?"),
        "saved": 0,
        "skipped": 0,
        "errors": 0,
        "cover": False,
    }

    # 1. Fetch metadata from source
    try:
        meta = await source.fetch_book_metadata(entry)
        if meta is None:
            log_detail(f"SKIP {book_id}: not found on source")
            stats["errors"] = -1
            return stats
        if meta.get("id") != book_id:
            log_detail(f"SKIP {book_id}: source ID mismatch (got {meta.get('id')})")
            stats["errors"] = -1
            return stats
        stats["name"] = meta.get("name", stats["name"])
    except Exception as e:
        log_detail(f"SKIP {book_id}: metadata error — {e}")
        stats["errors"] = -1
        return stats

    api_chapter_count = meta.get("chapter_count", 0)
    book_name = meta.get("name", "?")

    # 2. Determine what's needed — bundle-first skip logic
    bundle_path = str(COMPRESSED_DIR / f"{book_id}.bundle")
    bundle_indices = await asyncio.to_thread(read_bundle_indices, bundle_path)

    if (
        not fix_mode
        and len(bundle_indices) >= api_chapter_count
        and api_chapter_count > 0
    ):
        # Bundle is complete — check if DB needs update
        async with lock:
            db = open_db(db_path)
            try:
                existing_hash = get_book_meta_hash(db, book_id)
                meta_hash = compute_meta_hash(meta)
                db_indices = get_chapter_indices(db, book_id)
                missing_in_db = bundle_indices - db_indices
                need_meta_update = existing_hash != meta_hash

                if need_meta_update:
                    upsert_book_metadata(
                        db,
                        meta,
                        None,
                        len(bundle_indices),
                        meta_hash,
                        source=source.name,
                    )

                # Recover missing chapter rows from v2 bundle metadata
                if missing_in_db:
                    bundle_ch_meta = read_bundle_meta(bundle_path)
                    recover = {
                        idx: (m.title, m.slug, m.word_count, m.chapter_id)
                        for idx, m in bundle_ch_meta.items()
                        if idx in missing_in_db and m.title
                    }
                    if recover:
                        insert_chapters(db, book_id, recover)
                        log_detail(
                            f'RECOVER {book_id} "{book_name}": '
                            f"{len(recover)} chapter rows from bundle metadata"
                        )

                if need_meta_update or missing_in_db:
                    db.commit()

                if need_meta_update and not missing_in_db:
                    log_detail(
                        f'META {book_id} "{book_name}": metadata updated (chapters complete)'
                    )
            finally:
                db.close()

        # Pull cover if missing
        cover_url = await source.download_cover(book_id, meta, str(COVERS_DIR))
        if cover_url:
            async with lock:
                db = open_db(db_path)
                try:
                    update_cover_url(db, book_id, cover_url)
                    db.commit()
                finally:
                    db.close()
            stats["cover"] = True

        stats["skipped"] = len(bundle_indices)
        return stats

    # Bundle incomplete — query DB for chapter indices
    async with lock:
        db = open_db(db_path)
        try:
            db_indices = get_chapter_indices(db, book_id)
        finally:
            db.close()

    existing = bundle_indices | db_indices

    if not fix_mode and len(existing) >= api_chapter_count and api_chapter_count > 0:
        stats["skipped"] = len(existing)
        return stats

    if fix_mode and existing:
        gaps = api_chapter_count - len(existing) if api_chapter_count > 0 else 0
        log_detail(
            f'FIX {book_id} "{book_name}": {len(existing)} existing, '
            f"{api_chapter_count} total, ~{gaps} gaps to fill"
        )

    if source.name == "mtc" and not meta.get("first_chapter"):
        log_detail(f'SKIP {book_id} "{book_name}": no first_chapter')
        stats["errors"] = -1
        return stats

    if dry_run:
        remaining = api_chapter_count - len(existing)
        log_detail(f'DRY-RUN {book_id} "{book_name}": would fetch {remaining} chapters')
        stats["saved"] = remaining
        return stats

    # Ensure book row exists in DB before any chapter inserts (FK constraint)
    meta_hash = compute_meta_hash(meta)
    async with lock:
        db = open_db(db_path)
        try:
            if not get_book_meta_hash(db, book_id):
                upsert_book_metadata(
                    db,
                    meta,
                    None,
                    0,
                    meta_hash,
                    source=source.name,
                )
                db.commit()
        finally:
            db.close()

    # 3. Walk chapters via source (source handles walk strategy internally)
    log_detail(
        f'START {book_id} "{book_name}": {api_chapter_count} total, '
        f"{len(existing)} existing, ~{api_chapter_count - len(existing)} to fetch"
    )

    # pending: index -> (compressed, raw_len, title, slug, word_count, chapter_id)
    pending_chapters: dict[int, tuple[bytes, int, str, str, int, int]] = {}
    start_time = time.time()

    async for ch in source.fetch_chapters(meta, existing, bundle_path):
        compressed, raw_len = await asyncio.to_thread(compressor.compress, ch.body)
        pending_chapters[ch.index] = (
            compressed,
            raw_len,
            ch.title,
            ch.slug,
            ch.word_count,
            ch.chapter_id,
        )
        existing.add(ch.index)
        stats["saved"] += 1
        chapter_progress.update(chapter_task_id, advance=1)

        if len(pending_chapters) >= flush_every:
            await _flush_checkpoint(
                db_path, book_id, bundle_path, pending_chapters, lock
            )
            pending_chapters.clear()
            elapsed = time.time() - start_time
            rate = stats["saved"] / elapsed if elapsed > 0 else 0
            log_detail(
                f"  CHECKPOINT {book_id}[{ch.index}/{api_chapter_count}]: "
                f"+{stats['saved']} chapters ({rate:.1f}/s)"
            )

    # 4. Final flush
    if pending_chapters:
        await _flush_checkpoint(db_path, book_id, bundle_path, pending_chapters, lock)
        pending_chapters.clear()

    # 5. Update book metadata in DB (final — with cover + chapters_saved)
    total_saved = len(read_bundle_indices(bundle_path))

    # Pull cover
    cover_url = await source.download_cover(book_id, meta, str(COVERS_DIR))
    stats["cover"] = cover_url is not None

    async with lock:
        db = open_db(db_path)
        try:
            upsert_book_metadata(
                db,
                meta,
                cover_url,
                total_saved,
                meta_hash,
                source=source.name,
            )
            db.commit()
        finally:
            db.close()

    elapsed = time.time() - start_time
    rate = stats["saved"] / elapsed if elapsed > 0 else 0
    log_detail(
        f'DONE {book_id} "{book_name}": +{stats["saved"]} chapters '
        f"({total_saved} total), {stats['errors']} errors, "
        f"{elapsed:.0f}s ({rate:.1f}/s)"
        f"{', cover OK' if stats['cover'] else ''}"
    )

    return stats


async def _flush_checkpoint(
    db_path: str,
    book_id: int,
    bundle_path: str,
    pending: dict[int, tuple[bytes, int, str, str, int, int]],
    lock: asyncio.Lock,
) -> None:
    """Commit pending chapters to DB and merge into v2 bundle.

    DB transaction commits first; bundle flush follows.
    """
    # Prepare chapter metadata for DB (title, slug, word_count, chapter_id)
    ch_db_meta: dict[int, tuple[str, str, int, int]] = {}
    for idx, (_, _, title, slug, wc, ch_id) in pending.items():
        ch_db_meta[idx] = (title, slug, wc, ch_id)

    # Prepare compressed data + inline metadata for v2 bundle
    ch_data: dict[int, tuple[bytes, int]] = {}
    ch_bundle_meta: dict[int, ChapterMeta] = {}
    for idx, (compressed, raw_len, title, slug, wc, ch_id) in pending.items():
        ch_data[idx] = (compressed, raw_len)
        ch_bundle_meta[idx] = ChapterMeta(
            chapter_id=ch_id, word_count=wc, title=title, slug=slug
        )

    async with lock:
        # DB commit
        db = open_db(db_path)
        try:
            insert_chapters(db, book_id, ch_db_meta)
            db.commit()
        finally:
            db.close()

    # Bundle merge + write (can run outside lock — file is per-book)
    def _merge_and_write():
        existing_data = read_bundle_raw(bundle_path)
        existing_meta = read_bundle_meta(bundle_path)
        existing_data.update(ch_data)
        existing_meta.update(ch_bundle_meta)
        write_bundle(bundle_path, existing_data, existing_meta)

    await asyncio.to_thread(_merge_and_write)


# ─── Worker Pool ──────────────────────────────────────────────────────────────


async def run_ingest(
    entries: list[dict],
    workers: int,
    flush_every: int,
    dry_run: bool,
    source_name: str = "mtc",
    fix_mode: bool = False,
) -> None:
    """Run the ingest pipeline with a worker pool."""
    total_books = len(entries)
    db_path = str(DB_PATH)

    console.print(
        f"\n[bold]book-ingest[/bold] ({source_name}) — {format_num(total_books)} books, "
        f"{workers} workers, flush every {flush_every} chapters"
        f"{' [yellow](dry run)[/yellow]' if dry_run else ''}"
        f"{' [cyan](fix mode)[/cyan]' if fix_mode else ''}\n"
        f"  Workers: {workers}\n"
        f"  Source:  {source_name}\n"
        f"  DB:      {DB_PATH}\n"
        f"  Bundles: {COMPRESSED_DIR}\n"
        f"  Plan: {SCRIPT_DIR / 'data' / (PLAN_PREFIX + source_name + '.json')}  \n"
        f"  Covers:  {COVERS_DIR}\n"
        f"  Log:     {DETAIL_LOG}\n"
    )

    log_detail("=" * 60)
    log_detail(
        f"Ingest started — {total_books} books, workers={workers}"
        f"{', dry-run' if dry_run else ''}"
    )
    log_detail("=" * 60)

    # Init compressor
    compressor = ChapterCompressor(str(DICT_PATH))

    # Estimate total chapters from plan entries (enrich from DB if missing)
    missing_counts = [e for e in entries if not e.get("chapter_count")]
    if missing_counts:
        db = open_db(db_path)
        try:
            for e in missing_counts:
                row = db.execute(
                    "SELECT chapter_count FROM books WHERE id = ?", (e["id"],)
                ).fetchone()
                if row and row[0]:
                    e["chapter_count"] = row[0]
        finally:
            db.close()
    est_chapters = sum(e.get("chapter_count", 0) for e in entries)

    # ── Fix mode pre-audit: scan bundles for gaps ───────────────────────
    if fix_mode:
        console.print("[bold cyan]Fix mode: auditing bundles for gaps...[/bold cyan]")
        total_gaps = 0
        books_with_gaps = 0
        books_complete = 0
        # (id, name, bundle_count, expected_count, gap_count)
        audit_rows: list[tuple[int, str, int, int, int]] = []

        for e in entries:
            bid = e["id"]
            ch_count = e.get("chapter_count", 0)
            bpath = str(COMPRESSED_DIR / f"{bid}.bundle")
            b_indices = read_bundle_indices(bpath)
            expected = set(range(1, ch_count + 1)) if ch_count > 0 else set()
            gaps = len(expected - b_indices)
            if gaps > 0:
                books_with_gaps += 1
                total_gaps += gaps
                audit_rows.append(
                    (bid, e.get("name", "?"), len(b_indices), ch_count, gaps)
                )
            else:
                books_complete += 1

        console.print(f"  Books scanned:         {format_num(len(entries))}")
        console.print(f"  Already complete:      {format_num(books_complete)}")
        console.print(f"  With gaps:             {format_num(books_with_gaps)}")
        console.print(f"  Total chapters to fix: [bold]{format_num(total_gaps)}[/bold]")

        if audit_rows:
            audit_rows.sort(key=lambda r: -r[4])
            console.print(f"\n  [bold]Top books with gaps:[/bold]")
            for bid, name, have, total, gap in audit_rows[:15]:
                console.print(
                    f"    {bid:>10}  {have:>5}/{total:<5}  gap={gap:>5}  {name[:40]}"
                )
            if len(audit_rows) > 15:
                console.print(f"    [dim]... and {len(audit_rows) - 15} more[/dim]")

        if total_gaps == 0:
            console.print("\n[green]All books are complete. Nothing to fix.[/green]")
            return

        # Filter to only books with gaps; update totals for progress bar
        gap_book_ids = {r[0] for r in audit_rows}
        entries = [e for e in entries if e["id"] in gap_book_ids]
        total_books = len(entries)
        est_chapters = total_gaps
        console.print(
            f"\n  Proceeding with {format_num(total_books)} books, "
            f"{format_num(est_chapters)} chapters to download.\n"
        )

    start_time = time.time()
    lock = asyncio.Lock()  # protects DB access

    # Stats
    total_saved = 0
    total_skipped = 0
    total_errors = 0
    total_covers = 0
    books_processed = 0
    progress_interval = max(10, total_books // 20)

    # Queue of entries
    queue: asyncio.Queue[dict | None] = asyncio.Queue()
    for entry in entries:
        queue.put_nowait(entry)
    # Sentinel values for workers
    for _ in range(workers):
        queue.put_nowait(None)

    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        MofNCompleteColumn(),
        TaskProgressColumn(),
        TimeElapsedColumn(),
        TimeRemainingColumn(),
        console=console,
    ) as progress:
        book_task = progress.add_task("[cyan]Books", total=total_books)
        ch_label = "[cyan]Fixing" if fix_mode else "[green]Chapters"
        chapter_task = progress.add_task(ch_label, total=max(est_chapters, 1))

        async def worker():
            nonlocal total_saved, total_skipped, total_errors, total_covers
            nonlocal books_processed

            src_cfg = _SOURCE_DEFAULTS.get(source_name, _SOURCE_DEFAULTS["mtc"])
            mc = max(5, src_cfg["max_concurrent"] // workers)
            source = create_source(
                source_name,
                max_concurrent=mc,
                request_delay=src_cfg["request_delay"],
            )
            async with source:
                while True:
                    entry = await queue.get()
                    if entry is None:
                        break

                    try:
                        stats = await ingest_book(
                            source=source,
                            entry=entry,
                            compressor=compressor,
                            db_path=db_path,
                            flush_every=flush_every,
                            dry_run=dry_run,
                            book_progress=progress,
                            book_task_id=book_task,
                            chapter_progress=progress,
                            chapter_task_id=chapter_task,
                            lock=lock,
                            fix_mode=fix_mode,
                        )
                        total_saved += max(stats["saved"], 0)
                        total_skipped += max(stats["skipped"], 0)
                        total_errors += max(stats["errors"], 0)
                        if stats.get("cover"):
                            total_covers += 1
                    except Exception as e:
                        log_detail(f"FAIL {entry['id']}: {e}")
                        total_errors += 1

                    books_processed += 1
                    progress.update(book_task, advance=1)

                    # Periodic progress log
                    if books_processed % progress_interval == 0:
                        elapsed = time.time() - start_time
                        pct = (books_processed / total_books) * 100
                        books_per_sec = books_processed / elapsed if elapsed > 0 else 0
                        remaining = (
                            (total_books - books_processed) / books_per_sec
                            if books_per_sec > 0
                            else 0
                        )
                        log_detail(
                            f"PROGRESS: {format_num(books_processed)}/{format_num(total_books)} "
                            f"books ({pct:.1f}%), +{format_num(total_saved)} chapters, "
                            f"elapsed {format_duration(elapsed)}, "
                            f"ETA {format_duration(remaining)}"
                        )

        # Launch workers
        tasks = [asyncio.create_task(worker()) for _ in range(workers)]
        await asyncio.gather(*tasks)

    # Summary
    elapsed = time.time() - start_time

    log_detail("=" * 60)
    log_detail(
        f"COMPLETED: {total_books} books, {format_num(total_saved)} chapters added, "
        f"{format_num(total_skipped)} skipped, {total_errors} errors, "
        f"{total_covers} covers"
    )
    log_detail(f"Duration: {format_duration(elapsed)}")
    log_detail("=" * 60 + "\n")

    log_summary(
        f"ingest workers={workers} duration={format_duration(elapsed)} "
        f"books={total_books} chapters={total_saved} "
        f"skipped={total_skipped} covers={total_covers} errors={total_errors}"
    )

    console.print(
        f"\n[bold]Completed[/bold] in {format_duration(elapsed)}\n"
        f"  Books:    {format_num(total_books)}\n"
        f"  Chapters: +{format_num(total_saved)} new, "
        f"{format_num(total_skipped)} skipped\n"
        f"  Covers:   {total_covers}\n"
        f"  Errors:   {total_errors}\n"
    )


# ─── CLI ──────────────────────────────────────────────────────────────────────


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(
        description="book-ingest: unified crawl-decrypt-compress-import pipeline",
    )
    parser.add_argument(
        "book_ids",
        nargs="*",
        type=int,
        help="Specific book IDs to ingest (overrides plan file)",
    )
    parser.add_argument(
        "--source",
        choices=list(VALID_SOURCES),
        default="mtc",
        help="Data source: mtc (metruyencv, default) or ttv (tangthuvien)",
    )
    parser.add_argument(
        "-w",
        "--workers",
        type=int,
        default=5,
        help="Number of parallel workers (default: 5)",
    )
    parser.add_argument(
        "--plan",
        type=str,
        default=None,
        help="Path to plan JSON file (default: books_plan_mtc.json or books_plan_ttv.json)",
    )
    parser.add_argument(
        "--flush-every",
        type=int,
        default=100,
        help="Checkpoint interval in chapters (default: 100)",
    )
    parser.add_argument(
        "--audit-only",
        action="store_true",
        help="Audit mode: report missing data without downloading",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Simulate without writing any data",
    )
    parser.add_argument(
        "--offset",
        type=int,
        default=0,
        help="Skip first N entries in plan file",
    )
    parser.add_argument(
        "--limit",
        type=int,
        default=0,
        help="Limit to N entries from plan file (0 = all)",
    )
    parser.add_argument(
        "--min-chapters",
        type=int,
        default=100,
        help="Skip books with fewer than N chapters in the plan (default: 100). "
        "Ignored when explicit book IDs are given.",
    )
    parser.add_argument(
        "--update-meta-only",
        action="store_true",
        help="Update book metadata (author, stats, genres, tags) from the plan "
        "file without downloading chapters.  For books without an author, a "
        "synthetic author is generated from the creator (id=999{creator_id}).  "
        "Only processes books that already exist in the DB.",
    )
    parser.add_argument(
        "--force",
        action="store_true",
        help="Delete existing bundle + DB chapters and re-download from scratch. "
        "Requires explicit book IDs (not a plan file). Prompts for confirmation.",
    )
    parser.add_argument(
        "--fix",
        action="store_true",
        help="Re-download missing chapters for existing books. "
        "Audits each bundle for gaps and fills them. "
        "Works with plan files and explicit book IDs, all sources.",
    )
    return parser.parse_args()


def main():
    args = parse_args()

    # Validate paths
    if not DB_PATH.exists():
        console.print(f"[red]Error:[/red] Database not found: {DB_PATH}")
        sys.exit(1)
    if not DICT_PATH.exists():
        console.print(f"[red]Error:[/red] Zstd dictionary not found: {DICT_PATH}")
        sys.exit(1)
    COMPRESSED_DIR.mkdir(parents=True, exist_ok=True)
    COVERS_DIR.mkdir(parents=True, exist_ok=True)

    # Build entries list
    source_name = args.source
    if source_name == "ttv":
        default_plan = TTV_DEFAULT_PLAN
    elif source_name == "tf":
        default_plan = TF_DEFAULT_PLAN
    else:
        default_plan = DEFAULT_PLAN

    if args.book_ids:
        entries = entries_from_ids(args.book_ids, source_name)
    elif args.plan:
        entries = load_plan(args.plan, args.offset, args.limit)
    elif default_plan.exists():
        entries = load_plan(str(default_plan), args.offset, args.limit)
    else:
        console.print(
            "[red]Error:[/red] No book IDs provided and no plan file found.\n"
            f"  Expected: {default_plan}\n"
            "  Use: python3 ingest.py 100358  or  --plan path/to/plan.json"
        )
        sys.exit(1)

    # Filter out small books (only when loading from a plan, not explicit IDs)
    if not args.book_ids and args.min_chapters > 0:
        before = len(entries)
        entries = [e for e in entries if e.get("chapter_count", 0) >= args.min_chapters]
        skipped = before - len(entries)
        if skipped:
            console.print(
                f"[dim]Filtered out {skipped:,} books with < {args.min_chapters} chapters "
                f"({len(entries):,} remaining)[/dim]"
            )

    if not entries:
        console.print("[yellow]No entries to process.[/yellow]")
        sys.exit(0)

    # ── --force: delete existing data and re-download ───────────────────
    if args.force:
        if not args.book_ids:
            console.print(
                "[red]Error:[/red] --force requires explicit book IDs.\n"
                "  Example: python3 ingest.py --force 100358 100441\n"
                "  Using --force with a plan file is not supported (too dangerous)."
            )
            sys.exit(1)

        if args.dry_run or args.audit_only or args.update_meta_only:
            console.print(
                "[red]Error:[/red] --force cannot be combined with "
                "--dry-run, --audit-only, or --update-meta-only."
            )
            sys.exit(1)

        # Show what will be deleted
        db = open_db(str(DB_PATH))
        console.print(
            f"\n[bold red]⚠ FORCE RE-DOWNLOAD[/bold red] — "
            f"the following {len(entries)} book(s) will have their "
            f"bundle files and DB chapter rows [bold]deleted[/bold]:\n"
        )
        for e in entries:
            bid = e["id"]
            bpath = COMPRESSED_DIR / f"{bid}.bundle"
            bundle_exists = bpath.exists()
            bundle_size = bpath.stat().st_size if bundle_exists else 0
            ch_count = db.execute(
                "SELECT COUNT(*) FROM chapters WHERE book_id = ?", (bid,)
            ).fetchone()[0]
            name_row = db.execute(
                "SELECT name FROM books WHERE id = ?", (bid,)
            ).fetchone()
            name = name_row[0] if name_row else "?"
            console.print(
                f"  {bid}  {name[:50]}"
                f"  — bundle {'%.1fMB' % (bundle_size / 1048576) if bundle_exists else '[dim]none[/dim]'}"
                f", {ch_count:,} chapter rows"
            )
        db.close()

        console.print()
        try:
            answer = input("Type 'y' to confirm deletion and re-download: ").strip()
        except (KeyboardInterrupt, EOFError):
            console.print("\n[yellow]Aborted.[/yellow]")
            sys.exit(0)

        if answer.lower() != "y":
            console.print("[yellow]Aborted.[/yellow]")
            sys.exit(0)

        # Delete bundle files and DB chapter rows
        db = open_db(str(DB_PATH))
        for e in entries:
            bid = e["id"]
            bpath = COMPRESSED_DIR / f"{bid}.bundle"
            if bpath.exists():
                os.remove(bpath)
            db.execute("DELETE FROM chapters WHERE book_id = ?", (bid,))
            # Reset chapters_saved so metadata reflects the wipe
            db.execute("UPDATE books SET chapters_saved = 0 WHERE id = ?", (bid,))
        db.commit()
        db.close()
        console.print(
            f"[green]Deleted data for {len(entries)} book(s). "
            f"Starting re-download...[/green]\n"
        )

    # --fix bypasses min-chapters filter (we want to fix existing books)
    fix_mode = getattr(args, "fix", False)
    if fix_mode and not args.book_ids and args.min_chapters > 0:
        # In fix mode from plan, include all books regardless of chapter count
        # (they're already ingested, we just want to fill gaps)
        pass  # min-chapters filter was already applied above; re-include filtered
        entries = (
            load_plan(args.plan or str(default_plan), args.offset, args.limit)
            if not args.book_ids
            else entries
        )

    if args.update_meta_only:
        # Bare book IDs (no plan metadata) would produce garbage in
        # _plan_entry_to_meta() — name="?", slug="", chapter_count=0, no
        # author.  Enrich from the plan file so we get full metadata.
        if args.book_ids:
            plan_path = args.plan or (
                str(default_plan) if default_plan.exists() else None
            )
            if not plan_path:
                console.print(
                    "[red]Error:[/red] --update-meta-only with book IDs requires "
                    "a plan file.\n"
                    f"  Expected: {default_plan}\n"
                    "  Run generate_plan.py first, or use --plan path/to/plan.json"
                )
                sys.exit(1)
            plan_entries = load_plan(plan_path)
            plan_by_id = {e["id"]: e for e in plan_entries}
            enriched: list[dict] = []
            for e in entries:
                full = plan_by_id.get(e["id"])
                if full:
                    enriched.append(full)
                else:
                    console.print(
                        f"[yellow]Warning:[/yellow] Book {e['id']} not found "
                        "in plan file, skipping"
                    )
            entries = enriched
            if not entries:
                console.print(
                    "[yellow]No matching entries found in plan file.[/yellow]"
                )
                sys.exit(0)
        run_update_meta(entries)
    elif args.audit_only:
        asyncio.run(_run_audit_with_client(entries))
    else:
        asyncio.run(
            run_ingest(
                entries,
                args.workers,
                args.flush_every,
                args.dry_run,
                source_name,
                fix_mode=fix_mode,
            )
        )


def _plan_entry_to_meta(entry: dict) -> dict:
    """Reshape a plan file entry into the meta dict format expected by upsert_book_metadata.

    Plan entries from refresh_catalog.py have keys like author, creator, genres, etc.
    upsert_book_metadata expects a dict matching the API response shape.
    """
    meta: dict = {
        "id": entry["id"],
        "name": entry.get("name", "?"),
        "slug": entry.get("slug", ""),
        "synopsis": entry.get("synopsis"),
        "status": entry.get("status_code") or 1,
        "status_name": entry.get("status"),
        "view_count": entry.get("view_count", 0),
        "comment_count": entry.get("comment_count", 0),
        "bookmark_count": entry.get("bookmark_count", 0),
        "vote_count": entry.get("vote_count", 0),
        "review_score": entry.get("review_score", 0),
        "review_count": entry.get("review_count", 0),
        "chapter_count": entry.get("chapter_count", 0),
        "word_count": entry.get("word_count", 0),
        "created_at": entry.get("created_at"),
        "updated_at": entry.get("updated_at"),
        "published_at": entry.get("published_at"),
        "new_chap_at": entry.get("new_chap_at"),
    }

    # Author — apply fix-author logic: generate from creator if missing,
    # empty/whitespace, or a placeholder like "Đang cập nhật"
    author = entry.get("author")
    creator = entry.get("creator")
    _author_name = str(author.get("name", "")).strip() if author else ""
    author_needs_fix = (
        not author or not _author_name or _author_name.lower() == "đang cập nhật"
    )
    if author_needs_fix and creator and creator.get("id"):
        author = {
            "id": int(f"999{creator['id']}"),
            "name": creator.get("name", ""),
            "local_name": None,
            "avatar": None,
        }
    if author:
        meta["author"] = author

    # Genres
    meta["genres"] = entry.get("genres") or []

    # Tags
    meta["tags"] = entry.get("tags") or []

    return meta


def run_update_meta(entries: list[dict]) -> None:
    """Update book metadata in the DB from plan file entries.

    Only updates books that already exist in the DB.  Generates synthetic
    authors from creators for books without an author.
    """
    from src.db import compute_meta_hash, open_db, upsert_book_metadata

    db_path = str(DB_PATH)
    conn = open_db(db_path)

    # Get set of book IDs already in the DB
    existing_ids: set[int] = set()
    cur = conn.execute("SELECT id FROM books")
    for row in cur:
        existing_ids.add(row[0])

    total = len(entries)
    updated = 0
    skipped_not_in_db = 0
    skipped_no_change = 0
    authors_generated = 0
    errors = 0

    console.print(
        f"[bold]Updating metadata for {total:,} plan entries "
        f"({len(existing_ids):,} books in DB)[/bold]\n"
    )

    start = time.time()

    for i, entry in enumerate(entries):
        book_id = entry.get("id")
        if not book_id:
            errors += 1
            continue

        if book_id not in existing_ids:
            skipped_not_in_db += 1
            continue

        try:
            meta = _plan_entry_to_meta(entry)
            meta_hash = compute_meta_hash(meta)

            # Check if author was generated from creator
            original_author = entry.get("author")
            creator = entry.get("creator")
            _orig_name = (
                str(original_author.get("name", "")).strip() if original_author else ""
            )
            author_was_fixed = (
                not original_author
                or not _orig_name
                or _orig_name.lower() == "đang cập nhật"
            )
            if author_was_fixed and creator and creator.get("id"):
                authors_generated += 1

            # Preserve existing cover_url and chapters_saved
            row = conn.execute(
                "SELECT cover_url, chapters_saved FROM books WHERE id = ?",
                (book_id,),
            ).fetchone()
            cover_url = row[0] if row else None
            chapters_saved = row[1] if row else 0

            upsert_book_metadata(
                conn, meta, cover_url, chapters_saved, meta_hash, source="mtc"
            )
            updated += 1
        except Exception as e:
            log_detail(f"  ERROR {book_id}: {e}")
            errors += 1
            continue

        if (i + 1) % 1000 == 0:
            conn.commit()
            elapsed = time.time() - start
            rate = (i + 1) / elapsed if elapsed > 0 else 0
            console.print(
                f"  [{i + 1:,}/{total:,}] {rate:.0f}/s — "
                f"{updated:,} updated, {skipped_not_in_db:,} not in DB"
            )

    conn.commit()
    conn.close()

    elapsed = time.time() - start
    console.print()
    console.print("[bold]" + "=" * 60 + "[/bold]")
    console.print(f"  [bold]METADATA UPDATE COMPLETE[/bold]")
    console.print("=" * 60)
    console.print(f"  Plan entries      : {total:>10,}")
    console.print(f"  Updated           : {updated:>10,}")
    console.print(f"  Not in DB (skip)  : {skipped_not_in_db:>10,}")
    console.print(f"  Authors generated : {authors_generated:>10,}")
    console.print(f"  Errors            : {errors:>10,}")
    console.print(f"  Duration          : {elapsed:>10.1f}s")
    if elapsed > 0:
        console.print(f"  Rate              : {updated / elapsed:>10.0f} books/s")
    console.print("=" * 60)


async def _run_audit_with_client(entries: list[dict]) -> None:
    async with AsyncBookClient(max_concurrent=30, request_delay=0.1) as client:
        await run_audit(entries, client)


if __name__ == "__main__":
    main()
